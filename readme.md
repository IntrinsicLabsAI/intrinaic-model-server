[![LICENSE](https://img.shields.io/github/license/intrinsiclabsai/intrinsic-model-server)](LICENSE)
[![Python application](https://github.com/IntrinsicLabsAI/intrinsic-model-server/actions/workflows/python-app.yml/badge.svg)](https://github.com/IntrinsicLabsAI/intrinsic-model-server/actions/workflows/python-app.yml)
[![Frontend build and lint](https://github.com/IntrinsicLabsAI/intrinsic-model-server/actions/workflows/frontend.yml/badge.svg)](https://github.com/IntrinsicLabsAI/intrinsic-model-server/actions/workflows/frontend.yml)
[![Create and publish a Docker image](https://github.com/IntrinsicLabsAI/intrinsic-model-server/actions/workflows/docker.yml/badge.svg)](https://github.com/IntrinsicLabsAI/intrinsic-model-server/actions/workflows/docker.yml)


<img src="docs/app.jpg" style="display:block; margin:auto; width:auto"  />

# Intrinsic Model server

A simple API server on top of your favorite locally runnable foundation models. We support

* llama.cpp compatible models, including k-quant models
  * ARM NEON CPUs, BLAS, (CUDA coming soon)
* Whisper transcription models (COMING SOON)
* Visual models for object detection and segmentation (COMING SOON)


[Checkout our documentation!](https://intrinsiclabsai.github.io/intrinsic-model-server)

## Supported model types

- [x] LLM completion
- [ ] Transcription (coming soon)

